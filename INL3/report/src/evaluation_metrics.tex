\section{Metrics used for evaulation}
To compare the models, we have been given evaluation metrics, including accuracy and the F1 score, to assess their performance. 
Accuracy, ranging from $[0, 1]$, measures the proportion of correct predictions.
The F1 score balances precision and recall, combining them into a single metric that reflects the trade-off between false positives and false negatives.
\par 
Additionally, to visualize our true and false predictions, we have been provided with a function that plots a heatmap, giving us valuable insights into true/false positives and true/false negatives.