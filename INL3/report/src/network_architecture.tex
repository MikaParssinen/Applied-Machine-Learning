\section{Model architecture}

\subsection{Artificial neural network}
Our ANN model is built with the following architecture:
\begin{python}
model = Sequential()
model.add( 
         Dense(128, 
               input_shape=(784, ),
               activation="relu", 
               kernel_regularizer=l2(0.001)
               )
          )
model.add(Dense(64, activation="relu", kernel_regularizer=l2(0.001)))
model.add(Dropout(0.5))
model.add(Dense(32, activation="relu", kernel_regularizer=l2(0.001)))
model.add(Dense(10, activation="softmax"))
\end{python}
The ANN model is made by 4 layers: 1 input layer, 2 hidden layers, and 1 output layer. 
The input layer, with 128 neurons, uses the ReLU activation function and an L2 kernel regularizer (0.001) to prevent overfitting.
It also defines the input shape (784, ), which is representing a flattened 28x28 pixel image.
\par
The hidden layers extract increasingly complex information. 
The first hidden layer has 64 neurons, and the second has 32, both using ReLU and the same regularizer. 
A dropout function is included to deactivate neurons randomly, reducing overfitting.
\par
The output layer has 10 neurons (one per class) and uses the softmax activation function to predict the correct digit (0-9) from handwritten input.
The chosen architecture balances computational efficiency and accuracy. ReLU introduces non-linearity, enabling learning of complex patterns while being computationally efficient. 
Finally the L2 regularizer further reduces overfitting by penalizing large weights.

\begin{python}
optimizer = Adam(learning_rate=0.001)

model.compile(
    optimizer = optimizer, 
    loss = "categorical_crossentropy", 
    metrics = ["accuracy"]
)
\end{python}
To compile it we use the optimizer Adam with the learning rate of 0.001, and the loss function categorical crossentropy.
Adam is our choice because it adapts the learing rate for each parameter and is very computational friendly.
Lastly, we use the loss function because we have a multi classification problem.

\subsection{Convolutional neural network}

Our CNN model is built with the following architecture:

\begin{python}
regularizer = L2(1e-4)

model = Sequential()
model.add(Conv2D(32, (3, 3), activation="relu", input_shape=(28, 28, 1), use_bias=True, kernel_regularizer=regularizer))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.20))

model.add(Conv2D(32, (3, 3), activation='relu', use_bias=True, kernel_regularizer=regularizer))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.20))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.20))
model.add(Dense(10, activation='softmax'))
\end{python}

The model consists of two convolutional layers, each followed by a max pooling layer and a dropout layer. The output of the second max pooling layer is then flattened and passed through three dense layers. The first two dense layers have 256 and 512 neurons, respectively, and the last dense layer has 10 neurons. 
\par
Another dropout layer is added after the second dense layer to prevent overfitting. The activation function used in the convolutional layers is ReLU, and the output layer uses the softmax activation function. We use L2 regularization for the kernel in the convolutional layers a regularization factor of $1 \cdot 10^{-4}$.
\par
Both convolutional layers have 32 filters with a kernel size of $3 \times 3$ and they also use a bias term. The max pooling layers have a pool size of $2 \times 2$. Each dropout layer has a dropout rate of $0.20$.
\par
Our model is then compiled and fit in the following way:

\begin{python}
optimizer = Adam(learning_rate=0.0005)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3)
history = model.fit(x_train, y_train, epochs=20, validation_split=0.2, callbacks=[reduce_lr])
\end{python}

We compile the model with the Adam optimizer that has a learning rate of $0.0005$ and the loss function is categorical crossentropy. The metric for evaluation is accuracy.
\par
We also use a callback function that reduces the learning rate by a factor of $0.2$ if the validation loss does not improve for $3$ epochs. The model is then trained for 20 epochs with a validation split of $0.2$. 