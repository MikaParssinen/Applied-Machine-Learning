\section{Hyperparameter selection}
Different classifiers use different hyperparameters in their model, some have many different hyperparameters and some have none at all. 
This section explains choices made for each classifier when it comes to hyperparameters.

\subsection{Support vector machine (SVM)}

An SVM has several hyperparameters, but two of the most important are the Regularization Parameter 
(\(C\)) and the choice of Kernel. Depending on the kernel function used, other hyperparameters, such as Gamma 
(\(\gamma\)) and Degree, may also need to be tuned. Values chosen:

\begin{itemize}
    \item \textbf{\(\boldsymbol{C}\)}: 100
    \item \textbf{Kernel}: Radial Basis function (RBF)
    \item \textbf{\(\gamma\)}: 0.03571429
\end{itemize}
We utilized \textbf{Grid Search} to explore a wide range of hyperparameter values and then applied cross-validation to identify the model that achieved the best accuracy. 
The RBF kernel effectively captures non-linear relationships within the data, making it perfect for handling complex patterns. Difference in accuracy compared to the linear kernel was marginal but the runtime when using RBF was significant faster.
The regularization parameter \(C = 100\) increases the complexity of the decision boundary by dealing a higher penalty on misclassifications, ensuring that the model prioritizes accuracy on the training data. This hyperparameter played a crucial role in increasing the model's performance, as it provided a large boost in accuracy. 
Additionally, the parameter \(\gamma = \frac{1}{28} = 0.03571429\), comes from the formula \(\gamma = \frac{1}{d}\), where \(d\) represents the number of features in the dataset.

\subsection{Naive bayes classifiers}
Not all naive bayes classifiers have a hyperparameter, but some have a smoothing parameter $\alpha$. The values for $\alpha$ have been chosen by performing a grid search for classifiers with this hyperparameter. Since bernoulli naive bayes assumes all features being binary/boolean, a binarization threshold does also need to be chosen. 
\par
That being said, the classifier most well suited for our purpose is the Gaussian naive bayes classifier. This is because features can be assumed coming from a normal (Gaussian) distribution. The Gaussian naive bayes classifier do not have any hyperparameters, so we have no hyperparameter to tune. 

\subsection{Random forest}
The chosen hyperparameters for the Random Forest model are as follows:

\begin{itemize}
    \item \textbf{n\_estimators = 100:} Due to our limited computational resources, we have chosen to use 100 trees in the forest. While increasing the number of estimators can improve model performance, we found out that 100 trees gives us a reasonable balance between accuracy and computation time. In a perfect world, a larger number of estimators would be preferred.
    
    \item \textbf{max\_depth = 20:} Prevents the model from becoming too complex, which reduces the risk of overfitting. A depth that is too shallow may lead to underfitting, so we found that a depth of 20 gives a good balance for our dataset.
    
    \item \textbf{min\_samples\_split = 10:} By setting it to 10, we prevent the model from creating overly specific branches that may not generalize well, eliminating the risk of overfitting.
    
    \item \textbf{min\_samples\_leaf = 4:} This forces the model to make decisions based on a larger subset of data, improving generalization and reducing overfitting. 
    
    \item \textbf{max\_features = sqrt:} Commonly used setting for classification tasks, as it prevents trees becoming too correlated.
    
    \item \textbf{bootstrap = True:} Ensures that each tree is trained on a random subset of the data which may include duplicates. This makes each tree diverse and helps reduce overfitting and improving the overall robustness of the model.
\end{itemize}

We used \textbf{Grid Search} to tune these hyperparameters, testing different combinations to identify the best values that also provided the best accuracy for our model.