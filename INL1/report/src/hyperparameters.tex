\section{Hyperparameter selection}
Different classifiers use different hyperparameters in their model, some have many different hyperparameters and some have none at all. 
This section explains choices made for each classifier when it comes to hyperparameters.
We utilized \textbf{Grid Search} to test each hyperparameter value for each classifier. The best combinations were selected based on cross-validation, which allowed us to capture the best accuracy for the model.

\subsection{Support vector machine (SVM)}

The chosen hyperparameters for the SVM model are as follows:

\begin{itemize}
    \item \textbf{Regularization parameter (C) = 100:} Controls the trade-off between achieving low error on the training data and a low model complexity. A higher value of \(C\) minimizes training errors, which may increase the risk of overfitting. \(C = 100\) gives a good balance, providing high accuracy without overfitting.
    
    \item \textbf{Kernel = RBF:} The Radial Basis Function (RBF) kernel was chosen to capture the non-linear relationships between the data points. RBF is particularly effective in complex datasets where linear boundaries cannot separate the classes. Other kernel types, such as the linear kernel, were tested, but the RBF kernel performed better in terms of accuracy while also reducing runtime.
    
    \item \textbf{gamma = 0.03571429:} The parameter \(\gamma\) controls the influence of individual data points on the decision boundary. A value of \(\gamma = \frac{1}{28} = 0.03571429\) was chosen, which corresponds to the formula \(\gamma = \frac{1}{d}\), where \(d\) is the number of features in the dataset. This value provided good model performance without overfitting.
\end{itemize}

\subsection{Naive bayes classifiers}
Not all naive bayes classifiers have a hyperparameter, but some have a smoothing parameter $\alpha$. The values for $\alpha$ have been chosen by performing a grid search for classifiers with this hyperparameter. Since bernoulli naive bayes assumes all features being binary/boolean, a binarization threshold does also need to be chosen. 
\par
That being said, the classifier most well suited for our purpose is the Gaussian naive bayes classifier. This is because features can be assumed coming from a normal (Gaussian) distribution. The Gaussian naive bayes classifier do not have any hyperparameters, so we have no hyperparameter to tune. 

\subsection{Random forest}
The chosen hyperparameters for the Random Forest model are as follows:

\begin{itemize}
    \item \textbf{n\_estimators = 100:} Due to our limited computational resources, we have chosen to use 100 trees in the forest. While increasing the number of estimators can improve model performance, we found out that 100 trees gives us a reasonable balance between accuracy and computation time. In a perfect world, a larger number of estimators would be preferred.
    
    \item \textbf{max\_depth = 20:} Prevents the model from becoming too complex, which reduces the risk of overfitting. A depth that is too shallow may lead to underfitting, so we found that a depth of 20 gives a good balance for our dataset.
    
    \item \textbf{min\_samples\_split = 10:} By setting it to 10, we prevent the model from creating overly specific branches that may not generalize well, eliminating the risk of overfitting.
    
    \item \textbf{min\_samples\_leaf = 4:} This forces the model to make decisions based on a larger subset of data, improving generalization and reducing overfitting. 
    
    \item \textbf{max\_features = sqrt:} Commonly for classification tasks, as it prevents trees becoming too correlated.
    
    \item \textbf{bootstrap = True:} Ensures that each tree is trained on a random subset of the data which may include duplicates. This makes each tree diverse and helps reduce overfitting and improving the overall robustness of the model.
\end{itemize}
